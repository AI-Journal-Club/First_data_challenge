{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10ac4c0-b2a4-4086-a53f-9172445d9da8",
   "metadata": {},
   "source": [
    "# Oracle-MNIST Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446ee83",
   "metadata": {},
   "source": [
    "To run this you need to have the package 2dl installed: \n",
    "https://d2l.ai/chapter_installation/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c294c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils import images_train, labels_train, images_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15007071",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001 # Learning Rate\n",
    "BATCH_SIZE_TRAINING = 256\n",
    "BATCH_SIZE_VALIDATION = 36\n",
    "NUM_EPOCHS = 10\n",
    "TRAIN_VAL_SPLIT = 0.8\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d0d19",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785c624",
   "metadata": {},
   "source": [
    "Read more about LeNet (LeCun et al 1998): https://d2l.ai/chapter_convolutional-neural-networks/lenet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f02530c-2400-4afd-9433-b370fbe71c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(module):  #@save\n",
    "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        # nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.normal_(module.weight)\n",
    "\n",
    "class LeNet(d2l.Classifier):  #@save\n",
    "    \"\"\"The LeNet-5 model.\"\"\"\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        \n",
    "    def layer_summary(self, X_shape):\n",
    "        X = torch.randn(*X_shape)\n",
    "        for layer in self.net:\n",
    "            X = layer(X)\n",
    "            print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3a94549",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f067420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape:\t torch.Size([28, 28])\n",
      "Input tensor shape:\t torch.Size([1, 28, 28])\n",
      "Conv2d output shape:\t torch.Size([6, 28, 28])\n",
      "Sigmoid output shape:\t torch.Size([6, 28, 28])\n",
      "AvgPool2d output shape:\t torch.Size([6, 14, 14])\n",
      "Conv2d output shape:\t torch.Size([16, 10, 10])\n",
      "Sigmoid output shape:\t torch.Size([16, 10, 10])\n",
      "AvgPool2d output shape:\t torch.Size([16, 5, 5])\n",
      "Flatten output shape:\t torch.Size([16, 25])\n",
      "Linear output shape:\t torch.Size([16, 120])\n",
      "Sigmoid output shape:\t torch.Size([16, 120])\n",
      "Linear output shape:\t torch.Size([16, 84])\n",
      "Sigmoid output shape:\t torch.Size([16, 84])\n",
      "Linear output shape:\t torch.Size([16, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read one train image and check the layer output shapes\n",
    "image_example = torch.from_numpy(images_train[0])\n",
    "print('Input image shape:\\t', image_example.size())\n",
    "\n",
    "image_example = image_example.unsqueeze(0)  # Add batch dimension and the example\n",
    "print('Input tensor shape:\\t', image_example.size())\n",
    "\n",
    "model.layer_summary(image_example.shape)\n",
    "\n",
    "model.net.apply(init_cnn)\n",
    "\n",
    "# plot the distribution of weights before training\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(model.net[0].weight.data.numpy().flatten(), bins=30)\n",
    "plt.title('Weight distribution before training')\n",
    "plt.xlabel('Weight values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025ded2",
   "metadata": {},
   "source": [
    "# Define a DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf246093",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_tensor = torch.tensor(images_train)\n",
    "label_train_tensor = torch.tensor(labels_train)\n",
    "image_test_tensor = torch.tensor(images_test)\n",
    "label_test_tensor = torch.tensor(labels_test)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "dataset = TensorDataset(image_train_tensor, label_train_tensor)\n",
    "train_size = int(TRAIN_VAL_SPLIT * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE_TRAINING, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=BATCH_SIZE_VALIDATION, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682d489",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [6, 1, 5, 5], expected input[1, 256, 28, 28] to have 1 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m, in \u001b[0;36mLeNet.layer_summary\u001b[0;34m(self, X_shape)\u001b[0m\n\u001b[1;32m     23\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39mX_shape)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet:\n\u001b[0;32m---> 25\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 1, 5, 5], expected input[1, 256, 28, 28] to have 1 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.layer_summary((256, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba37ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Training on 86 batches\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.Tensor'> torch.Size([256, 28, 28]) <class 'torch.Tensor'> torch.Size([256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [6, 1, 5, 5], expected input[1, 256, 28, 28] to have 1 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m data_ \u001b[38;5;241m=\u001b[39m data_\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, label_\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/d2l/torch.py:193\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeural network is defined\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 1, 5, 5], expected input[1, 256, 28, 28] to have 1 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "train_loss_ = []\n",
    "val_loss_ = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(f\"Training on {len(trainloader)} batches\")\n",
    "    print(type(trainloader))\n",
    "    for data_, label_ in trainloader:\n",
    "        print(type(data_), data_.shape, type(label_), label_.shape)\n",
    "        optimizer.zero_grad()\n",
    "        data_ = data_.to(dtype=torch.float, device=device)\n",
    "        outputs = model(data_)\n",
    "        loss = criterion(outputs, label_.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    # if SCHEDULER:\n",
    "    #     scheduler.step()\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_loss_.append(train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    print(f\"Validating on {len(valloader)} batches\")\n",
    "    with torch.no_grad():\n",
    "        for data_, label_ in valloader:\n",
    "            data_ = data_.to(dtype=torch.float, device=device)\n",
    "            outputs = model(data_)\n",
    "            loss = criterion(outputs, label_.to(device))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(valloader)\n",
    "    val_loss_.append(val_loss)\n",
    "    \n",
    "    # --- Logging ---\n",
    "    if epoch%2 == 0:\n",
    "        print(f\"epoch : {epoch}/{NUM_EPOCHS}; Validation loss = {round(val_loss, 6)}; Training loss = {round(train_loss, 6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4144b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
